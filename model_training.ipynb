{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq6X8gHGWE6H",
        "outputId": "43750747-3945-4ed9-e602-bdd39f8d1cb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DI_nTRUWMh1",
        "outputId": "ffd159e6-ef77-4fc5-9fcd-863d81774bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas==1.3.3\n",
            "  Downloading pandas-1.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.3) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.3) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.3) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.0.0 requires pandas>=1.5.0, but you have pandas 1.3.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.3.3 which is incompatible.\n",
            "mizani 0.9.3 requires pandas>=1.3.5, but you have pandas 1.3.3 which is incompatible.\n",
            "plotnine 0.12.4 requires pandas>=1.5.0, but you have pandas 1.3.3 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas==1.3.3\n",
        "import pandas as pd\n",
        "# print(pd.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeHVbEtRWXix",
        "outputId": "902f3c8e-4c5c-4728-dddc-4b2ebb788043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Group-01\n",
            "Group-03\n",
            "Group-04\n",
            "Group-04\n",
            "Group-01\n",
            "Group-01\n",
            "Group-01\n",
            "Group-02\n",
            "Group-02\n",
            "Group-04\n",
            "Group-01\n",
            "Group-01\n",
            "Group-04\n",
            "Group-06\n",
            "Group-01\n",
            "Group-01\n",
            "Group-02\n",
            "Group-03\n",
            "Group-04\n",
            "Group-06\n",
            "Group-06\n",
            "Group-03\n",
            "Group-03\n",
            "Group-03\n",
            "Group-04\n",
            "Group-07\n",
            "Group-01\n",
            "Group-01\n",
            "Group-01\n",
            "Group-01\n",
            "Group-02\n",
            "Group-02\n",
            "Group-02\n",
            "Group-03\n",
            "Group-03\n",
            "Group-03\n",
            "Group-04\n",
            "Group-04\n",
            "Group-04\n",
            "Group-04\n",
            "Group-01\n",
            "Group-04\n",
            "Group-05\n",
            "Group-05\n",
            "Group-05\n",
            "Group-06\n",
            "Group-01\n",
            "Group-02\n",
            "Group-02\n",
            "Group-07\n",
            "Group-08\n",
            "Group-05\n",
            "Group-06\n",
            "Group-08\n",
            "Group-08\n",
            "Group-02\n",
            "Group-02\n",
            "Group-02\n",
            "Group-03\n",
            "Group-03\n",
            "Group-03\n",
            "Group-03\n",
            "Group-04\n",
            "Group-04\n",
            "Group-05\n",
            "Group-06\n",
            "Group-06\n",
            "Group-07\n",
            "Group-04\n",
            "Group-04\n",
            "Group-05\n",
            "Group-01\n",
            "Group-01\n",
            "Group-02\n",
            "Group-07\n",
            "Group-08\n",
            "Group-01\n",
            "Group-01\n",
            "Group-02\n",
            "Group-03\n",
            "Group-03\n",
            "Group-06\n",
            "Group-07\n",
            "Group-01\n",
            "Group-01\n",
            "Group-01\n",
            "Group-01\n",
            "Group-02\n",
            "Group-02\n",
            "Group-02\n",
            "Group-03\n",
            "Group-03\n",
            "Group-06\n",
            "Group-07\n",
            "Group-07\n",
            "Group-04\n",
            "Group-05\n",
            "Group-05\n",
            "Group-05\n",
            "Group-01\n",
            "Group-01\n",
            "Group-01\n",
            "Group-02\n",
            "Group-02\n",
            "Group-02\n",
            "Group-02\n",
            "Group-05\n",
            "Group-08\n",
            "Group-01\n",
            "Group-04\n",
            "Group-06\n",
            "Group-06\n",
            "Group-06\n",
            "Group-01\n",
            "Group-01\n",
            "Group-01\n",
            "Group-01\n",
            "Group-01\n",
            "Group-02\n",
            "Group-02\n",
            "Group-02\n",
            "Group-02\n",
            "Group-03\n",
            "Group-03\n",
            "Group-04\n",
            "Group-05\n",
            "Group-06\n",
            "Group-02\n",
            "Group-02\n",
            "Group-02\n",
            "Group-02\n",
            "Group-03\n",
            "Group-04\n",
            "Group-04\n",
            "Group-04\n",
            "Group-04\n",
            "Group-07\n",
            "Group-07\n",
            "Group-05\n",
            "Group-07\n",
            "Group-01\n",
            "Group-02\n",
            "Group-02\n",
            "Group-02\n",
            "Group-03\n",
            "Group-03\n",
            "Group-03\n",
            "Group-05\n",
            "Group-06\n",
            "Group-06\n",
            "Group-04\n",
            "Group-04\n",
            "Group-05\n",
            "Group-06\n",
            "Group-01\n",
            "Group-03\n",
            "Group-05\n",
            "Group-08\n",
            "Group-01\n",
            "Group-01\n",
            "Group-01\n",
            "Group-02\n",
            "Group-03\n",
            "Group-03\n",
            "Group-05\n",
            "Group-05\n",
            "Group-07\n",
            "Group-07\n",
            "Group-02\n",
            "Group-02\n",
            "Group-02\n",
            "Group-06\n",
            "Group-03\n",
            "Group-04\n",
            "Group-06\n",
            "Group-08\n",
            "Group-03\n",
            "Group-04\n",
            "Group-05\n",
            "Group-05\n",
            "Group-07\n",
            "Group-08\n",
            "Group-08\n",
            "Group-08\n",
            "Group-01\n",
            "Group-02\n",
            "Group-02\n",
            "Group-08\n",
            "Group-01\n",
            "Group-02\n",
            "Group-04\n",
            "Group-06\n",
            "Group-06\n",
            "Group-07\n",
            "Group-08\n",
            "Group-02\n",
            "Group-05\n",
            "Group-06\n",
            "Group-07\n",
            "Group-08\n",
            "Group-01\n",
            "Group-03\n",
            "Group-03\n",
            "Group-05\n",
            "Group-05\n",
            "Group-07\n",
            "Group-01\n",
            "Group-01\n",
            "Group-02\n",
            "Group-02\n",
            "Group-04\n",
            "Group-05\n",
            "Group-08\n",
            "Group-01\n",
            "Group-01\n",
            "Group-03\n",
            "Group-07\n",
            "Group-01\n",
            "Group-02\n",
            "Group-02\n",
            "Group-02\n",
            "Group-03\n",
            "Group-03\n",
            "Group-03\n",
            "Group-03\n",
            "Group-03\n",
            "Group-04\n",
            "Group-05\n",
            "Group-05\n",
            "Group-05\n",
            "Group-06\n",
            "Group-01\n",
            "Group-01\n",
            "Group-02\n",
            "Group-07\n",
            "Group-07\n",
            "Group-08\n",
            "Group-08\n",
            "Group-01\n",
            "Group-04\n",
            "Group-06\n",
            "Group-06\n",
            "Group-08\n",
            "Group-01\n",
            "Group-01\n",
            "Group-03\n",
            "Group-03\n",
            "Group-05\n",
            "Group-06\n",
            "Group-07\n",
            "Group-08\n",
            "Group-08\n",
            "Group-01\n",
            "Group-01\n",
            "Group-02\n",
            "Group-04\n",
            "Group-04\n",
            "Group-04\n",
            "Group-05\n",
            "Group-05\n",
            "Group-06\n",
            "Group-03\n",
            "Group-03\n",
            "Group-05\n",
            "Group-06\n",
            "Group-07\n",
            "Group-02\n",
            "Group-02\n",
            "Group-04\n",
            "Group-07\n",
            "Group-07\n",
            "Group-08\n",
            "Group-08\n",
            "31\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the path to the folder containing the files\n",
        "folder_path = '/content/drive/MyDrive/dense/89_Features/'\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dataframes_list = [[] for i in range(31)]\n",
        "labels = []\n",
        "V = []\n",
        "A = []\n",
        "D = []\n",
        "L = []\n",
        "F = []\n",
        "R = []\n",
        "\n",
        "emotion_labels_path = '/content/drive/MyDrive/dense/cleaned_data_readings_valence.csv'\n",
        "emotion_labels_data = pd.read_csv(emotion_labels_path)\n",
        "\n",
        "# Iterate through each file in the folder\n",
        "for index, row1 in emotion_labels_data.iterrows():\n",
        "    file_name = row1['file_name']\n",
        "    emotion_label = row1['Groups']\n",
        "    print(emotion_label)\n",
        "    val = row1['V']\n",
        "    aro = row1['A']\n",
        "    dom = row1['D']\n",
        "    lik = row1['L']\n",
        "    fam = row1['F']\n",
        "    rel = row1['R']\n",
        "\n",
        "    # if file_name.endswith('.csv'):  # Adjust the file extension accordingly\n",
        "    file_path = os.path.join(folder_path, f'{file_name}_eegfeatures1.csv')\n",
        "    labels.append(emotion_label)\n",
        "    V.append(val)\n",
        "    A.append(aro)\n",
        "    D.append(dom)\n",
        "    L.append(lik)\n",
        "    F.append(fam)\n",
        "    R.append(rel)\n",
        "\n",
        "    # Read the file into a DataFrame\n",
        "    df = pd.read_csv(file_path,index_col=0)  # Adjust the delimiter accordingly\n",
        "\n",
        "    # Iterate through each row in the DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        # Create a DataFrame for each row\n",
        "        row_df = pd.DataFrame(row).T\n",
        "\n",
        "        # Append the row DataFrame to the list\n",
        "        dataframes_list[index].append(row_df)\n",
        "\n",
        "# Concatenate all DataFrames in the list into a single DataFrame\n",
        "dataframes_file = []\n",
        "for i in range(31):\n",
        "    final_dataframe = pd.concat(dataframes_list[i], ignore_index=True)\n",
        "    dataframes_file.append(final_dataframe)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(len(dataframes_file))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3vR4tMiWuHS"
      },
      "outputs": [],
      "source": [
        "Y_v = pd.DataFrame(V)\n",
        "Y_a = pd.DataFrame(A)\n",
        "Y_labels = pd.DataFrame(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r8ly5dSWvA-",
        "outputId": "eba4ab59-2ebd-4813-d40c-5da1c7c17a96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy.stats import ttest_rel\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "Y_v = label_encoder.fit_transform(Y_v)\n",
        "Y_a = label_encoder.fit_transform(Y_a)\n",
        "# Y_va = label_encoder.fit_transform(Y_va)\n",
        "Y_labels = label_encoder.fit_transform(Y_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQswgHW_mG65"
      },
      "outputs": [],
      "source": [
        "col = dataframes_file[0].columns[20:25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9sGvxWYKwUN",
        "outputId": "b1f806d0-4c71-47c0-ae37-7125424aec31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['PSD_delta', 'PSD_theta', 'PSD_alpha', 'PSD_beta', 'PSD_gamma'], dtype='object')"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmJ9nkz-mxKz"
      },
      "outputs": [],
      "source": [
        "for i in range(len(dataframes_file)):\n",
        "  dataframes_file[i] = dataframes_file[i].drop(columns = col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2oXYH91Kf6g",
        "outputId": "ecce4487-b47b-4bf2-d428-19870d372f99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataframes_file[0].columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Opwy9UVjW0Hw",
        "outputId": "1b66c8d3-57fe-46b9-f95a-f2064cbaf5d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1]\n",
            "Accuracy(HALA): 0.6\n",
            "Precision: 0.6198830409356726\n",
            "Recall: 0.6087533156498675\n",
            "F1 Score: 0.5934139784946236\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.45      0.54        29\n",
            "           1       0.56      0.77      0.65        26\n",
            "\n",
            "    accuracy                           0.60        55\n",
            "   macro avg       0.62      0.61      0.59        55\n",
            "weighted avg       0.62      0.60      0.59        55\n",
            "\n",
            "['0.62', '0.60', '0.59', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1]\n",
            "Accuracy(HALA): 0.45454545454545453\n",
            "Precision: 0.45906432748538006\n",
            "Recall: 0.46286472148541113\n",
            "F1 Score: 0.4455645161290323\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.31      0.38        29\n",
            "           1       0.44      0.62      0.52        26\n",
            "\n",
            "    accuracy                           0.45        55\n",
            "   macro avg       0.46      0.46      0.45        55\n",
            "weighted avg       0.46      0.45      0.44        55\n",
            "\n",
            "['0.46', '0.45', '0.44', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0]\n",
            "Accuracy(HALA): 0.509090909090909\n",
            "Precision: 0.512768817204301\n",
            "Recall: 0.5125994694960212\n",
            "F1 Score: 0.5084409136047666\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.45      0.49        29\n",
            "           1       0.48      0.58      0.53        26\n",
            "\n",
            "    accuracy                           0.51        55\n",
            "   macro avg       0.51      0.51      0.51        55\n",
            "weighted avg       0.51      0.51      0.51        55\n",
            "\n",
            "['0.51', '0.51', '0.51', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1]\n",
            "Accuracy(HALA): 0.4909090909090909\n",
            "Precision: 0.49719887955182074\n",
            "Recall: 0.4973474801061008\n",
            "F1 Score: 0.4866666666666667\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.38      0.44        29\n",
            "           1       0.47      0.62      0.53        26\n",
            "\n",
            "    accuracy                           0.49        55\n",
            "   macro avg       0.50      0.50      0.49        55\n",
            "weighted avg       0.50      0.49      0.48        55\n",
            "\n",
            "['0.50', '0.49', '0.48', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 1]\n",
            "Accuracy(HALA): 0.5818181818181818\n",
            "Precision: 0.5866935483870968\n",
            "Recall: 0.5855437665782494\n",
            "F1 Score: 0.5812644819596161\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.52      0.57        29\n",
            "           1       0.55      0.65      0.60        26\n",
            "\n",
            "    accuracy                           0.58        55\n",
            "   macro avg       0.59      0.59      0.58        55\n",
            "weighted avg       0.59      0.58      0.58        55\n",
            "\n",
            "['0.59', '0.58', '0.58', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1]\n",
            "Accuracy(HALA): 0.4909090909090909\n",
            "Precision: 0.5015479876160991\n",
            "Recall: 0.5013262599469496\n",
            "F1 Score: 0.47690217391304346\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.31      0.39        29\n",
            "           1       0.47      0.69      0.56        26\n",
            "\n",
            "    accuracy                           0.49        55\n",
            "   macro avg       0.50      0.50      0.48        55\n",
            "weighted avg       0.50      0.49      0.47        55\n",
            "\n",
            "['0.50', '0.49', '0.47', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1]\n",
            "Accuracy(HALA): 0.6181818181818182\n",
            "Precision: 0.6733449477351916\n",
            "Recall: 0.6319628647214854\n",
            "F1 Score: 0.5990975355779242\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.38      0.51        29\n",
            "           1       0.56      0.88      0.69        26\n",
            "\n",
            "    accuracy                           0.62        55\n",
            "   macro avg       0.67      0.63      0.60        55\n",
            "weighted avg       0.68      0.62      0.59        55\n",
            "\n",
            "['0.68', '0.62', '0.59', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1]\n",
            "Accuracy(HALA): 0.509090909090909\n",
            "Precision: 0.5151515151515151\n",
            "Recall: 0.5145888594164456\n",
            "F1 Score: 0.5064805583250249\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.41      0.47        29\n",
            "           1       0.48      0.62      0.54        26\n",
            "\n",
            "    accuracy                           0.51        55\n",
            "   macro avg       0.52      0.51      0.51        55\n",
            "weighted avg       0.52      0.51      0.50        55\n",
            "\n",
            "['0.52', '0.51', '0.50', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 0]\n",
            "Accuracy(HALA): 0.5636363636363636\n",
            "Precision: 0.5641534391534391\n",
            "Recall: 0.5643236074270557\n",
            "F1 Score: 0.5634920634920635\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.55      0.57        29\n",
            "           1       0.54      0.58      0.56        26\n",
            "\n",
            "    accuracy                           0.56        55\n",
            "   macro avg       0.56      0.56      0.56        55\n",
            "weighted avg       0.57      0.56      0.56        55\n",
            "\n",
            "['0.57', '0.56', '0.56', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0]\n",
            "Accuracy(HALA): 0.5454545454545454\n",
            "Precision: 0.5409356725146199\n",
            "Recall: 0.5371352785145889\n",
            "F1 Score: 0.5299145299145299\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.69      0.62        29\n",
            "           1       0.53      0.38      0.44        26\n",
            "\n",
            "    accuracy                           0.55        55\n",
            "   macro avg       0.54      0.54      0.53        55\n",
            "weighted avg       0.54      0.55      0.53        55\n",
            "\n",
            "['0.54', '0.55', '0.53', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0]\n",
            "Accuracy(HALA): 0.6181818181818182\n",
            "Precision: 0.6166666666666667\n",
            "Recall: 0.6160477453580901\n",
            "F1 Score: 0.6161515453639083\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.66      0.64        29\n",
            "           1       0.60      0.58      0.59        26\n",
            "\n",
            "    accuracy                           0.62        55\n",
            "   macro avg       0.62      0.62      0.62        55\n",
            "weighted avg       0.62      0.62      0.62        55\n",
            "\n",
            "['0.62', '0.62', '0.62', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0]\n",
            "Accuracy(HALA): 0.41818181818181815\n",
            "Precision: 0.4201680672268907\n",
            "Recall: 0.4244031830238727\n",
            "F1 Score: 0.41333333333333333\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.31      0.36        29\n",
            "           1       0.41      0.54      0.47        26\n",
            "\n",
            "    accuracy                           0.42        55\n",
            "   macro avg       0.42      0.42      0.41        55\n",
            "weighted avg       0.42      0.42      0.41        55\n",
            "\n",
            "['0.42', '0.42', '0.41', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0]\n",
            "Accuracy(HALA): 0.5454545454545454\n",
            "Precision: 0.5433333333333333\n",
            "Recall: 0.5431034482758621\n",
            "F1 Score: 0.5430375540046527\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.59      0.58        29\n",
            "           1       0.52      0.50      0.51        26\n",
            "\n",
            "    accuracy                           0.55        55\n",
            "   macro avg       0.54      0.54      0.54        55\n",
            "weighted avg       0.54      0.55      0.54        55\n",
            "\n",
            "['0.54', '0.55', '0.54', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "Accuracy(HALA): 0.5818181818181818\n",
            "Precision: 0.5800000000000001\n",
            "Recall: 0.5795755968169761\n",
            "F1 Score: 0.5795945496842805\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.62      0.61        29\n",
            "           1       0.56      0.54      0.55        26\n",
            "\n",
            "    accuracy                           0.58        55\n",
            "   macro avg       0.58      0.58      0.58        55\n",
            "weighted avg       0.58      0.58      0.58        55\n",
            "\n",
            "['0.58', '0.58', '0.58', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0]\n",
            "Accuracy(HALA): 0.5818181818181818\n",
            "Precision: 0.5800000000000001\n",
            "Recall: 0.5795755968169761\n",
            "F1 Score: 0.5795945496842805\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.62      0.61        29\n",
            "           1       0.56      0.54      0.55        26\n",
            "\n",
            "    accuracy                           0.58        55\n",
            "   macro avg       0.58      0.58      0.58        55\n",
            "weighted avg       0.58      0.58      0.58        55\n",
            "\n",
            "['0.58', '0.58', '0.58', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0]\n",
            "Accuracy(HALA): 0.5636363636363636\n",
            "Precision: 0.5611559139784946\n",
            "Recall: 0.5603448275862069\n",
            "F1 Score: 0.56\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.62      0.60        29\n",
            "           1       0.54      0.50      0.52        26\n",
            "\n",
            "    accuracy                           0.56        55\n",
            "   macro avg       0.56      0.56      0.56        55\n",
            "weighted avg       0.56      0.56      0.56        55\n",
            "\n",
            "['0.56', '0.56', '0.56', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0]\n",
            "Accuracy(HALA): 0.5454545454545454\n",
            "Precision: 0.5775261324041812\n",
            "Recall: 0.5590185676392573\n",
            "F1 Score: 0.5227351614022909\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.31      0.42        29\n",
            "           1       0.51      0.81      0.63        26\n",
            "\n",
            "    accuracy                           0.55        55\n",
            "   macro avg       0.58      0.56      0.52        55\n",
            "weighted avg       0.58      0.55      0.52        55\n",
            "\n",
            "['0.58', '0.55', '0.52', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0\n",
            " 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1]\n",
            "Accuracy(HALA): 0.509090909090909\n",
            "Precision: 0.512768817204301\n",
            "Recall: 0.5125994694960212\n",
            "F1 Score: 0.5084409136047666\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.45      0.49        29\n",
            "           1       0.48      0.58      0.53        26\n",
            "\n",
            "    accuracy                           0.51        55\n",
            "   macro avg       0.51      0.51      0.51        55\n",
            "weighted avg       0.51      0.51      0.51        55\n",
            "\n",
            "['0.51', '0.51', '0.51', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0\n",
            " 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0]\n",
            "Accuracy(HALA): 0.4909090909090909\n",
            "Precision: 0.4914021164021164\n",
            "Recall: 0.4913793103448276\n",
            "F1 Score: 0.49074074074074076\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.48      0.50        29\n",
            "           1       0.46      0.50      0.48        26\n",
            "\n",
            "    accuracy                           0.49        55\n",
            "   macro avg       0.49      0.49      0.49        55\n",
            "weighted avg       0.49      0.49      0.49        55\n",
            "\n",
            "['0.49', '0.49', '0.49', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0]\n",
            "Accuracy(HALA): 0.5272727272727272\n",
            "Precision: 0.53\n",
            "Recall: 0.5298408488063661\n",
            "F1 Score: 0.5271164021164021\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.48      0.52        29\n",
            "           1       0.50      0.58      0.54        26\n",
            "\n",
            "    accuracy                           0.53        55\n",
            "   macro avg       0.53      0.53      0.53        55\n",
            "weighted avg       0.53      0.53      0.53        55\n",
            "\n",
            "['0.53', '0.53', '0.53', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1]\n",
            "Accuracy(HALA): 0.5272727272727272\n",
            "Precision: 0.55\n",
            "Recall: 0.5397877984084881\n",
            "F1 Score: 0.5075757575757576\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.31      0.41        29\n",
            "           1       0.50      0.77      0.61        26\n",
            "\n",
            "    accuracy                           0.53        55\n",
            "   macro avg       0.55      0.54      0.51        55\n",
            "weighted avg       0.55      0.53      0.50        55\n",
            "\n",
            "['0.55', '0.53', '0.50', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0]\n",
            "Accuracy(HALA): 0.5454545454545454\n",
            "Precision: 0.5689102564102564\n",
            "Recall: 0.557029177718833\n",
            "F1 Score: 0.5299145299145299\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.34      0.44        29\n",
            "           1       0.51      0.77      0.62        26\n",
            "\n",
            "    accuracy                           0.55        55\n",
            "   macro avg       0.57      0.56      0.53        55\n",
            "weighted avg       0.57      0.55      0.53        55\n",
            "\n",
            "['0.57', '0.55', '0.53', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0]\n",
            "Accuracy(HALA): 0.43636363636363634\n",
            "Precision: 0.4393939393939394\n",
            "Recall: 0.4416445623342175\n",
            "F1 Score: 0.43336656696576936\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.34      0.39        29\n",
            "           1       0.42      0.54      0.47        26\n",
            "\n",
            "    accuracy                           0.44        55\n",
            "   macro avg       0.44      0.44      0.43        55\n",
            "weighted avg       0.44      0.44      0.43        55\n",
            "\n",
            "['0.44', '0.44', '0.43', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0]\n",
            "Accuracy(HALA): 0.5454545454545454\n",
            "Precision: 0.5409356725146199\n",
            "Recall: 0.5371352785145889\n",
            "F1 Score: 0.5299145299145299\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.69      0.62        29\n",
            "           1       0.53      0.38      0.44        26\n",
            "\n",
            "    accuracy                           0.55        55\n",
            "   macro avg       0.54      0.54      0.53        55\n",
            "weighted avg       0.54      0.55      0.53        55\n",
            "\n",
            "['0.54', '0.55', '0.53', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1\n",
            " 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 0]\n",
            "Accuracy(HALA): 0.6\n",
            "Precision: 0.6005291005291005\n",
            "Recall: 0.6007957559681698\n",
            "F1 Score: 0.5998677248677249\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.59      0.61        29\n",
            "           1       0.57      0.62      0.59        26\n",
            "\n",
            "    accuracy                           0.60        55\n",
            "   macro avg       0.60      0.60      0.60        55\n",
            "weighted avg       0.60      0.60      0.60        55\n",
            "\n",
            "['0.60', '0.60', '0.60', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1]\n",
            "Accuracy(HALA): 0.43636363636363634\n",
            "Precision: 0.4393939393939394\n",
            "Recall: 0.4416445623342175\n",
            "F1 Score: 0.43336656696576936\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.34      0.39        29\n",
            "           1       0.42      0.54      0.47        26\n",
            "\n",
            "    accuracy                           0.44        55\n",
            "   macro avg       0.44      0.44      0.43        55\n",
            "weighted avg       0.44      0.44      0.43        55\n",
            "\n",
            "['0.44', '0.44', '0.43', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1]\n",
            "Accuracy(HALA): 0.5272727272727272\n",
            "Precision: 0.5326086956521738\n",
            "Recall: 0.5318302387267905\n",
            "F1 Score: 0.5258620689655171\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.45      0.50        29\n",
            "           1       0.50      0.62      0.55        26\n",
            "\n",
            "    accuracy                           0.53        55\n",
            "   macro avg       0.53      0.53      0.53        55\n",
            "weighted avg       0.53      0.53      0.52        55\n",
            "\n",
            "['0.53', '0.53', '0.52', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0]\n",
            "Accuracy(HALA): 0.4909090909090909\n",
            "Precision: 0.4952445652173913\n",
            "Recall: 0.49535809018567634\n",
            "F1 Score: 0.4893899204244032\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.41      0.46        29\n",
            "           1       0.47      0.58      0.52        26\n",
            "\n",
            "    accuracy                           0.49        55\n",
            "   macro avg       0.50      0.50      0.49        55\n",
            "weighted avg       0.50      0.49      0.49        55\n",
            "\n",
            "['0.50', '0.49', '0.49', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0]\n",
            "Accuracy(HALA): 0.41818181818181815\n",
            "Precision: 0.41885964912280704\n",
            "Recall: 0.42639257294429705\n",
            "F1 Score: 0.4086021505376344\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.28      0.33        29\n",
            "           1       0.42      0.58      0.48        26\n",
            "\n",
            "    accuracy                           0.42        55\n",
            "   macro avg       0.42      0.43      0.41        55\n",
            "weighted avg       0.42      0.42      0.40        55\n",
            "\n",
            "['0.42', '0.42', '0.40', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1\n",
            " 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0]\n",
            "Accuracy(HALA): 0.5636363636363636\n",
            "Precision: 0.5611559139784946\n",
            "Recall: 0.5603448275862069\n",
            "F1 Score: 0.56\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.62      0.60        29\n",
            "           1       0.54      0.50      0.52        26\n",
            "\n",
            "    accuracy                           0.56        55\n",
            "   macro avg       0.56      0.56      0.56        55\n",
            "weighted avg       0.56      0.56      0.56        55\n",
            "\n",
            "['0.56', '0.56', '0.56', '55']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1]\n",
            "Accuracy(HALA): 0.45454545454545453\n",
            "Precision: 0.4589783281733746\n",
            "Recall: 0.46485411140583555\n",
            "F1 Score: 0.43953804347826086\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.28      0.35        29\n",
            "           1       0.45      0.65      0.53        26\n",
            "\n",
            "    accuracy                           0.45        55\n",
            "   macro avg       0.46      0.46      0.44        55\n",
            "weighted avg       0.46      0.45      0.43        55\n",
            "\n",
            "['0.46', '0.45', '0.43', '55']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "results_df_a = pd.DataFrame(columns=['Accuracy'])\n",
        "\n",
        "for i in range(31):\n",
        "    X_train, X_test, y_train_a, y_test_a = train_test_split(dataframes_file[i].iloc[:,:], Y_v, test_size=0.2, random_state=21, shuffle=True)\n",
        "    acc_val = []\n",
        "    for i in range(1,200):\n",
        "        rf = RandomForestClassifier(n_estimators=i)\n",
        "\n",
        "        # Train the classifier\n",
        "        rf.fit(X_train, y_train_a)\n",
        "\n",
        "        # Make predictions on the test set\n",
        "        y_pred = rf.predict(X_test)\n",
        "\n",
        "        accuracy_a = accuracy_score(y_test_a, y_pred)\n",
        "        acc_val.append(accuracy_a)\n",
        "\n",
        "    acc_val = np.asarray(acc_val)\n",
        "    index = acc_val.argmax()\n",
        "\n",
        "    # Create a Random Forest classifier\n",
        "    rf = RandomForestClassifier(n_estimators=index+1, random_state=21)\n",
        "\n",
        "    # Train the classifier\n",
        "    start_time = time.time()\n",
        "    rf.fit(X_train, y_train_a)\n",
        "    end_time = time.time()\n",
        "\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = rf.predict(X_test)\n",
        "\n",
        "    # Calculate the accuracy\n",
        "    accuracy_a = accuracy_score(y_test_a, y_pred)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test_a, y_pred, average='macro')\n",
        "\n",
        "    print(y_test_a, y_pred)\n",
        "    print(f\"Accuracy(HALA): {accuracy_a}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1_score}\")\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test_a, y_pred))\n",
        "\n",
        "    # Append the results to the results dataframe\n",
        "    report = classification_report(y_test_a, y_pred)\n",
        "    print(report.split('\\n')[-2].split()[2:6])\n",
        "    precision, recall, fscore, support = map(float, report.split('\\n')[-6].split()[1:5])\n",
        "    precisionhv, recallhv, fscorehv, support = map(float, report.split('\\n')[-7].split()[1:5])\n",
        "    precisionmv, recallmv, fscoremv, support = map(float, report.split('\\n')[-3].split()[2:6])\n",
        "    precisionwv, recallwv, fscorewv, support = map(float, report.split('\\n')[-2].split()[2:6])\n",
        "    results_df_a = results_df_a.append({'Accuracy': accuracy_a, 'n_estimators': index, 'time': execution_time, 'Precision[LA]': precision, 'Recall[LA]': recall, \"F-score[LA]\": fscore, 'Precision[HA]': precisionhv, 'Recall[HA]': recallhv, \"F-score[HA]\": fscorehv,'Precision[MA]':precisionmv, 'Recall[MA]':recallmv, 'F-score[MA]':fscoremv,'Precision[WA]':precisionwv, 'Recall[WA]':recallwv, 'F-score[WA]':fscorewv},ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XZzxFQY5W9lx",
        "outputId": "d0808cd8-487d-4fb5-cb99-e8af0ffa8fce"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"results_df_a\",\n  \"rows\": 31,\n  \"fields\": [\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05737217699657408,\n        \"min\": 0.41818181818181815,\n        \"max\": 0.6181818181818182,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.6181818181818182,\n          0.6,\n          0.5272727272727272\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F-score[HA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09466409204434723,\n        \"min\": 0.33,\n        \"max\": 0.64,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0.54,\n          0.41,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F-score[LA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0601163745964746,\n        \"min\": 0.44,\n        \"max\": 0.69,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.65,\n          0.52,\n          0.69\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F-score[MA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05790425913355988,\n        \"min\": 0.41,\n        \"max\": 0.62,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.62,\n          0.54,\n          0.59\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F-score[WA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05952861424540718,\n        \"min\": 0.4,\n        \"max\": 0.62,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.59,\n          0.44,\n          0.47\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision[HA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07764684319445651,\n        \"min\": 0.42,\n        \"max\": 0.79,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          0.68,\n          0.47,\n          0.59\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision[LA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05093111519124294,\n        \"min\": 0.41,\n        \"max\": 0.6,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.56,\n          0.44,\n          0.54\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision[MA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06112388266674545,\n        \"min\": 0.42,\n        \"max\": 0.67,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.62,\n          0.46,\n          0.67\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision[WA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06207349095684801,\n        \"min\": 0.42,\n        \"max\": 0.68,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.62,\n          0.46,\n          0.68\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall[HA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.133315052510234,\n        \"min\": 0.28,\n        \"max\": 0.69,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.34,\n          0.62,\n          0.45\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall[LA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11282682514862447,\n        \"min\": 0.38,\n        \"max\": 0.88,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.5,\n          0.62,\n          0.88\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall[MA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05670713292638927,\n        \"min\": 0.42,\n        \"max\": 0.63,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.61,\n          0.46,\n          0.63\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall[WA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.057310698207509725,\n        \"min\": 0.42,\n        \"max\": 0.62,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.62,\n          0.6,\n          0.53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_estimators\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18.59512018309255,\n        \"min\": 0.0,\n        \"max\": 85.0,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          17.0,\n          27.0,\n          12.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03312832210081531,\n        \"min\": 0.004194498062133789,\n        \"max\": 0.15810894966125488,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          0.014796018600463867,\n          0.004498481750488281,\n          0.006482362747192383\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "results_df_a"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-5259f6b9-3902-468b-9c6f-4eae666ca665\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F-score[HA]</th>\n",
              "      <th>F-score[LA]</th>\n",
              "      <th>F-score[MA]</th>\n",
              "      <th>F-score[WA]</th>\n",
              "      <th>Precision[HA]</th>\n",
              "      <th>Precision[LA]</th>\n",
              "      <th>Precision[MA]</th>\n",
              "      <th>Precision[WA]</th>\n",
              "      <th>Recall[HA]</th>\n",
              "      <th>Recall[LA]</th>\n",
              "      <th>Recall[MA]</th>\n",
              "      <th>Recall[WA]</th>\n",
              "      <th>n_estimators</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.60</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.032977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.45</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.054039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.509091</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.490909</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.49</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.026354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.581818</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.019639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.490909</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.49</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.025441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.618182</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.62</td>\n",
              "      <td>85.0</td>\n",
              "      <td>0.158109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.509091</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.51</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.019119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.563636</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.028963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.618182</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.62</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.035798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.418182</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.42</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.036501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.55</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.035794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.581818</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.021072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.581818</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.034205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.563636</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.55</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.079641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.509091</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.51</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.014204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.490909</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.527273</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.53</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.043629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.527273</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.53</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.036240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.55</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.060874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.436364</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.44</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.042757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.55</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.006482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.014187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.436364</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.44</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.015495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.527273</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.53</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.046966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.490909</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.49</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.014796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.418182</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.42</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.020174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.563636</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.023885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.45</td>\n",
              "      <td>66.0</td>\n",
              "      <td>0.119819</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5259f6b9-3902-468b-9c6f-4eae666ca665')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5259f6b9-3902-468b-9c6f-4eae666ca665 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5259f6b9-3902-468b-9c6f-4eae666ca665');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d20cf680-4d3d-4bc8-98c4-344ff118893d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d20cf680-4d3d-4bc8-98c4-344ff118893d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d20cf680-4d3d-4bc8-98c4-344ff118893d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    Accuracy  F-score[HA]  F-score[LA]  F-score[MA]  F-score[WA]  \\\n",
              "0   0.600000         0.54         0.65         0.59         0.59   \n",
              "1   0.454545         0.38         0.52         0.45         0.44   \n",
              "2   0.509091         0.49         0.53         0.51         0.51   \n",
              "3   0.490909         0.44         0.53         0.49         0.48   \n",
              "4   0.581818         0.57         0.60         0.58         0.58   \n",
              "5   0.490909         0.39         0.56         0.48         0.47   \n",
              "6   0.618182         0.51         0.69         0.60         0.59   \n",
              "7   0.509091         0.47         0.54         0.51         0.50   \n",
              "8   0.563636         0.57         0.56         0.56         0.56   \n",
              "9   0.545455         0.62         0.44         0.53         0.53   \n",
              "10  0.618182         0.64         0.59         0.62         0.62   \n",
              "11  0.418182         0.36         0.47         0.41         0.41   \n",
              "12  0.545455         0.58         0.51         0.54         0.54   \n",
              "13  0.581818         0.61         0.55         0.58         0.58   \n",
              "14  0.581818         0.61         0.55         0.58         0.58   \n",
              "15  0.563636         0.60         0.52         0.56         0.56   \n",
              "16  0.545455         0.42         0.63         0.52         0.52   \n",
              "17  0.509091         0.49         0.53         0.51         0.51   \n",
              "18  0.490909         0.50         0.48         0.49         0.49   \n",
              "19  0.527273         0.52         0.54         0.53         0.53   \n",
              "20  0.527273         0.41         0.61         0.51         0.50   \n",
              "21  0.545455         0.44         0.62         0.53         0.53   \n",
              "22  0.436364         0.39         0.47         0.43         0.43   \n",
              "23  0.545455         0.62         0.44         0.53         0.53   \n",
              "24  0.600000         0.61         0.59         0.60         0.60   \n",
              "25  0.436364         0.39         0.47         0.43         0.43   \n",
              "26  0.527273         0.50         0.55         0.53         0.52   \n",
              "27  0.490909         0.46         0.52         0.49         0.49   \n",
              "28  0.418182         0.33         0.48         0.41         0.40   \n",
              "29  0.563636         0.60         0.52         0.56         0.56   \n",
              "30  0.454545         0.35         0.53         0.44         0.43   \n",
              "\n",
              "    Precision[HA]  Precision[LA]  Precision[MA]  Precision[WA]  Recall[HA]  \\\n",
              "0            0.68           0.56           0.62           0.62        0.45   \n",
              "1            0.47           0.44           0.46           0.46        0.31   \n",
              "2            0.54           0.48           0.51           0.51        0.45   \n",
              "3            0.52           0.47           0.50           0.50        0.38   \n",
              "4            0.62           0.55           0.59           0.59        0.52   \n",
              "5            0.53           0.47           0.50           0.50        0.31   \n",
              "6            0.79           0.56           0.67           0.68        0.38   \n",
              "7            0.55           0.48           0.52           0.52        0.41   \n",
              "8            0.59           0.54           0.56           0.57        0.55   \n",
              "9            0.56           0.53           0.54           0.54        0.69   \n",
              "10           0.63           0.60           0.62           0.62        0.66   \n",
              "11           0.43           0.41           0.42           0.42        0.31   \n",
              "12           0.57           0.52           0.54           0.54        0.59   \n",
              "13           0.60           0.56           0.58           0.58        0.62   \n",
              "14           0.60           0.56           0.58           0.58        0.62   \n",
              "15           0.58           0.54           0.56           0.56        0.62   \n",
              "16           0.64           0.51           0.58           0.58        0.31   \n",
              "17           0.54           0.48           0.51           0.51        0.45   \n",
              "18           0.52           0.46           0.49           0.49        0.48   \n",
              "19           0.56           0.50           0.53           0.53        0.48   \n",
              "20           0.60           0.50           0.55           0.55        0.31   \n",
              "21           0.62           0.51           0.57           0.57        0.34   \n",
              "22           0.45           0.42           0.44           0.44        0.34   \n",
              "23           0.56           0.53           0.54           0.54        0.69   \n",
              "24           0.63           0.57           0.60           0.60        0.59   \n",
              "25           0.45           0.42           0.44           0.44        0.34   \n",
              "26           0.57           0.50           0.53           0.53        0.45   \n",
              "27           0.52           0.47           0.50           0.50        0.41   \n",
              "28           0.42           0.42           0.42           0.42        0.28   \n",
              "29           0.58           0.54           0.56           0.56        0.62   \n",
              "30           0.47           0.45           0.46           0.46        0.28   \n",
              "\n",
              "    Recall[LA]  Recall[MA]  Recall[WA]  n_estimators      time  \n",
              "0         0.77        0.61        0.60          17.0  0.032977  \n",
              "1         0.62        0.46        0.45          27.0  0.054039  \n",
              "2         0.58        0.51        0.51           0.0  0.006902  \n",
              "3         0.62        0.50        0.49          11.0  0.026354  \n",
              "4         0.65        0.59        0.58           9.0  0.019639  \n",
              "5         0.69        0.50        0.49          12.0  0.025441  \n",
              "6         0.88        0.63        0.62          85.0  0.158109  \n",
              "7         0.62        0.51        0.51           8.0  0.019119  \n",
              "8         0.58        0.56        0.56          13.0  0.028963  \n",
              "9         0.38        0.54        0.55           0.0  0.004194  \n",
              "10        0.58        0.62        0.62          17.0  0.035798  \n",
              "11        0.54        0.42        0.42          17.0  0.036501  \n",
              "12        0.50        0.54        0.55           9.0  0.035794  \n",
              "13        0.54        0.58        0.58           5.0  0.021072  \n",
              "14        0.54        0.58        0.58           9.0  0.034205  \n",
              "15        0.50        0.56        0.56           0.0  0.004498  \n",
              "16        0.81        0.56        0.55          38.0  0.079641  \n",
              "17        0.58        0.51        0.51           5.0  0.014204  \n",
              "18        0.50        0.49        0.49           0.0  0.005035  \n",
              "19        0.58        0.53        0.53          13.0  0.043629  \n",
              "20        0.77        0.54        0.53          18.0  0.036240  \n",
              "21        0.77        0.56        0.55          33.0  0.060874  \n",
              "22        0.54        0.44        0.44          22.0  0.042757  \n",
              "23        0.38        0.54        0.55           1.0  0.006482  \n",
              "24        0.62        0.60        0.60           5.0  0.014187  \n",
              "25        0.54        0.44        0.44           6.0  0.015495  \n",
              "26        0.62        0.53        0.53          13.0  0.046966  \n",
              "27        0.58        0.50        0.49           6.0  0.014796  \n",
              "28        0.58        0.43        0.42           8.0  0.020174  \n",
              "29        0.50        0.56        0.56          11.0  0.023885  \n",
              "30        0.65        0.46        0.45          66.0  0.119819  "
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QrdQfY_XDao"
      },
      "outputs": [],
      "source": [
        "ch_names = ['Cz','FP1','AF3','F7','F3','Fz','FC5','FC1','T7','C3','CP5','CP1','P7','P3','PO3','O1','FP2','AF4','F8','F4','FC6','FC2','T8','C4','CP6','CP2','P8','P4','Pz','PO4','O2','Oz']\n",
        "results_df_a['ch_names'] = ch_names[1:]\n",
        "results_df_a.to_csv('/content/drive/MyDrive/dense/rhythms_418_time/selected_v/84_features_valence_rf.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO5VW1hyXJ_k",
        "outputId": "54e3fc43-3bf9-4ce3-97c8-e2dd11238328"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1]\n",
            "Accuracy(HVLV): 0.6181818181818182\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.31      0.46        29\n",
            "           1       0.56      0.96      0.70        26\n",
            "\n",
            "    accuracy                           0.62        55\n",
            "   macro avg       0.73      0.64      0.58        55\n",
            "weighted avg       0.74      0.62      0.58        55\n",
            "\n",
            "['0.90', '0.31', '0.46', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0]\n",
            "Accuracy(HVLV): 0.5636363636363636\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.48      0.54        29\n",
            "           1       0.53      0.65      0.59        26\n",
            "\n",
            "    accuracy                           0.56        55\n",
            "   macro avg       0.57      0.57      0.56        55\n",
            "weighted avg       0.57      0.56      0.56        55\n",
            "\n",
            "['0.61', '0.48', '0.54', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1\n",
            " 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0]\n",
            "Accuracy(HVLV): 0.6545454545454545\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.83      0.72        29\n",
            "           1       0.71      0.46      0.56        26\n",
            "\n",
            "    accuracy                           0.65        55\n",
            "   macro avg       0.67      0.64      0.64        55\n",
            "weighted avg       0.67      0.65      0.64        55\n",
            "\n",
            "['0.63', '0.83', '0.72', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0]\n",
            "Accuracy(HVLV): 0.5818181818181818\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.45      0.53        29\n",
            "           1       0.54      0.73      0.62        26\n",
            "\n",
            "    accuracy                           0.58        55\n",
            "   macro avg       0.60      0.59      0.58        55\n",
            "weighted avg       0.60      0.58      0.57        55\n",
            "\n",
            "['0.65', '0.45', '0.53', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0]\n",
            "Accuracy(HVLV): 0.6363636363636364\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.38      0.52        29\n",
            "           1       0.57      0.92      0.71        26\n",
            "\n",
            "    accuracy                           0.64        55\n",
            "   macro avg       0.71      0.65      0.61        55\n",
            "weighted avg       0.72      0.64      0.61        55\n",
            "\n",
            "['0.85', '0.38', '0.52', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1\n",
            " 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0]\n",
            "Accuracy(HVLV): 0.5818181818181818\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.55      0.58        29\n",
            "           1       0.55      0.62      0.58        26\n",
            "\n",
            "    accuracy                           0.58        55\n",
            "   macro avg       0.58      0.58      0.58        55\n",
            "weighted avg       0.59      0.58      0.58        55\n",
            "\n",
            "['0.62', '0.55', '0.58', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1\n",
            " 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 0]\n",
            "Accuracy(HVLV): 0.5818181818181818\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.45      0.53        29\n",
            "           1       0.54      0.73      0.62        26\n",
            "\n",
            "    accuracy                           0.58        55\n",
            "   macro avg       0.60      0.59      0.58        55\n",
            "weighted avg       0.60      0.58      0.57        55\n",
            "\n",
            "['0.65', '0.45', '0.53', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1\n",
            " 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0]\n",
            "Accuracy(HVLV): 0.6363636363636364\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.59      0.63        29\n",
            "           1       0.60      0.69      0.64        26\n",
            "\n",
            "    accuracy                           0.64        55\n",
            "   macro avg       0.64      0.64      0.64        55\n",
            "weighted avg       0.64      0.64      0.64        55\n",
            "\n",
            "['0.68', '0.59', '0.63', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0]\n",
            "Accuracy(HVLV): 0.6\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.52      0.58        29\n",
            "           1       0.56      0.69      0.62        26\n",
            "\n",
            "    accuracy                           0.60        55\n",
            "   macro avg       0.61      0.60      0.60        55\n",
            "weighted avg       0.61      0.60      0.60        55\n",
            "\n",
            "['0.65', '0.52', '0.58', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1]\n",
            "Accuracy(HVLV): 0.5272727272727272\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.24      0.35        29\n",
            "           1       0.50      0.85      0.63        26\n",
            "\n",
            "    accuracy                           0.53        55\n",
            "   macro avg       0.57      0.54      0.49        55\n",
            "weighted avg       0.57      0.53      0.48        55\n",
            "\n",
            "['0.64', '0.24', '0.35', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0]\n",
            "Accuracy(HVLV): 0.6363636363636364\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.48      0.58        29\n",
            "           1       0.58      0.81      0.68        26\n",
            "\n",
            "    accuracy                           0.64        55\n",
            "   macro avg       0.66      0.65      0.63        55\n",
            "weighted avg       0.66      0.64      0.63        55\n",
            "\n",
            "['0.74', '0.48', '0.58', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0\n",
            " 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0]\n",
            "Accuracy(HVLV): 0.6\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.72      0.66        29\n",
            "           1       0.60      0.46      0.52        26\n",
            "\n",
            "    accuracy                           0.60        55\n",
            "   macro avg       0.60      0.59      0.59        55\n",
            "weighted avg       0.60      0.60      0.59        55\n",
            "\n",
            "['0.60', '0.72', '0.66', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1]\n",
            "Accuracy(HVLV): 0.6363636363636364\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.34      0.50        29\n",
            "           1       0.57      0.96      0.71        26\n",
            "\n",
            "    accuracy                           0.64        55\n",
            "   macro avg       0.74      0.65      0.61        55\n",
            "weighted avg       0.75      0.64      0.60        55\n",
            "\n",
            "['0.91', '0.34', '0.50', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 1]\n",
            "Accuracy(HVLV): 0.6181818181818182\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.52      0.59        29\n",
            "           1       0.58      0.73      0.64        26\n",
            "\n",
            "    accuracy                           0.62        55\n",
            "   macro avg       0.63      0.62      0.62        55\n",
            "weighted avg       0.63      0.62      0.61        55\n",
            "\n",
            "['0.68', '0.52', '0.59', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 0 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "Accuracy(HVLV): 0.5818181818181818\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.41      0.51        29\n",
            "           1       0.54      0.77      0.63        26\n",
            "\n",
            "    accuracy                           0.58        55\n",
            "   macro avg       0.60      0.59      0.57        55\n",
            "weighted avg       0.61      0.58      0.57        55\n",
            "\n",
            "['0.67', '0.41', '0.51', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1]\n",
            "Accuracy(HVLV): 0.5818181818181818\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.48      0.55        29\n",
            "           1       0.55      0.69      0.61        26\n",
            "\n",
            "    accuracy                           0.58        55\n",
            "   macro avg       0.59      0.59      0.58        55\n",
            "weighted avg       0.59      0.58      0.58        55\n",
            "\n",
            "['0.64', '0.48', '0.55', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0]\n",
            "Accuracy(HVLV): 0.5818181818181818\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.59      0.60        29\n",
            "           1       0.56      0.58      0.57        26\n",
            "\n",
            "    accuracy                           0.58        55\n",
            "   macro avg       0.58      0.58      0.58        55\n",
            "weighted avg       0.58      0.58      0.58        55\n",
            "\n",
            "['0.61', '0.59', '0.60', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
            "Accuracy(HVLV): 0.6\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.79      0.68        29\n",
            "           1       0.62      0.38      0.48        26\n",
            "\n",
            "    accuracy                           0.60        55\n",
            "   macro avg       0.61      0.59      0.58        55\n",
            "weighted avg       0.61      0.60      0.58        55\n",
            "\n",
            "['0.59', '0.79', '0.68', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1]\n",
            "Accuracy(HVLV): 0.5272727272727272\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.10      0.19        29\n",
            "           1       0.50      1.00      0.67        26\n",
            "\n",
            "    accuracy                           0.53        55\n",
            "   macro avg       0.75      0.55      0.43        55\n",
            "weighted avg       0.76      0.53      0.41        55\n",
            "\n",
            "['1.00', '0.10', '0.19', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0]\n",
            "Accuracy(HVLV): 0.6545454545454545\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.69      0.68        29\n",
            "           1       0.64      0.62      0.63        26\n",
            "\n",
            "    accuracy                           0.65        55\n",
            "   macro avg       0.65      0.65      0.65        55\n",
            "weighted avg       0.65      0.65      0.65        55\n",
            "\n",
            "['0.67', '0.69', '0.68', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1]\n",
            "Accuracy(HVLV): 0.6\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.48      0.56        29\n",
            "           1       0.56      0.73      0.63        26\n",
            "\n",
            "    accuracy                           0.60        55\n",
            "   macro avg       0.61      0.61      0.60        55\n",
            "weighted avg       0.62      0.60      0.59        55\n",
            "\n",
            "['0.67', '0.48', '0.56', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1]\n",
            "Accuracy(HVLV): 0.509090909090909\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.38      0.45        29\n",
            "           1       0.49      0.65      0.56        26\n",
            "\n",
            "    accuracy                           0.51        55\n",
            "   macro avg       0.52      0.52      0.50        55\n",
            "weighted avg       0.52      0.51      0.50        55\n",
            "\n",
            "['0.55', '0.38', '0.45', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1\n",
            " 1 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0]\n",
            "Accuracy(HVLV): 0.5818181818181818\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.66      0.62        29\n",
            "           1       0.57      0.50      0.53        26\n",
            "\n",
            "    accuracy                           0.58        55\n",
            "   macro avg       0.58      0.58      0.58        55\n",
            "weighted avg       0.58      0.58      0.58        55\n",
            "\n",
            "['0.59', '0.66', '0.62', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1]\n",
            "Accuracy(HVLV): 0.5636363636363636\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.34      0.45        29\n",
            "           1       0.53      0.81      0.64        26\n",
            "\n",
            "    accuracy                           0.56        55\n",
            "   macro avg       0.60      0.58      0.55        55\n",
            "weighted avg       0.60      0.56      0.54        55\n",
            "\n",
            "['0.67', '0.34', '0.45', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 1 0 0 0 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0]\n",
            "Accuracy(HVLV): 0.6545454545454545\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.66      0.67        29\n",
            "           1       0.63      0.65      0.64        26\n",
            "\n",
            "    accuracy                           0.65        55\n",
            "   macro avg       0.65      0.65      0.65        55\n",
            "weighted avg       0.66      0.65      0.65        55\n",
            "\n",
            "['0.68', '0.66', '0.67', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1\n",
            " 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1]\n",
            "Accuracy(HVLV): 0.5454545454545454\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.59      0.58        29\n",
            "           1       0.52      0.50      0.51        26\n",
            "\n",
            "    accuracy                           0.55        55\n",
            "   macro avg       0.54      0.54      0.54        55\n",
            "weighted avg       0.54      0.55      0.54        55\n",
            "\n",
            "['0.57', '0.59', '0.58', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1\n",
            " 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0]\n",
            "Accuracy(HVLV): 0.5636363636363636\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.62      0.60        29\n",
            "           1       0.54      0.50      0.52        26\n",
            "\n",
            "    accuracy                           0.56        55\n",
            "   macro avg       0.56      0.56      0.56        55\n",
            "weighted avg       0.56      0.56      0.56        55\n",
            "\n",
            "['0.58', '0.62', '0.60', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0]\n",
            "Accuracy(HVLV): 0.6\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.38      0.50        29\n",
            "           1       0.55      0.85      0.67        26\n",
            "\n",
            "    accuracy                           0.60        55\n",
            "   macro avg       0.64      0.61      0.58        55\n",
            "weighted avg       0.65      0.60      0.58        55\n",
            "\n",
            "['0.73', '0.38', '0.50', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0]\n",
            "Accuracy(HVLV): 0.6363636363636364\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.55      0.62        29\n",
            "           1       0.59      0.73      0.66        26\n",
            "\n",
            "    accuracy                           0.64        55\n",
            "   macro avg       0.64      0.64      0.64        55\n",
            "weighted avg       0.65      0.64      0.63        55\n",
            "\n",
            "['0.70', '0.55', '0.62', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0]\n",
            "Accuracy(HVLV): 0.5818181818181818\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.31      0.44        29\n",
            "           1       0.53      0.88      0.67        26\n",
            "\n",
            "    accuracy                           0.58        55\n",
            "   macro avg       0.64      0.60      0.55        55\n",
            "weighted avg       0.65      0.58      0.55        55\n",
            "\n",
            "['0.75', '0.31', '0.44', '29']\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0\n",
            " 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "Accuracy(HVLV): 0.5818181818181818\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.59      0.60        29\n",
            "           1       0.56      0.58      0.57        26\n",
            "\n",
            "    accuracy                           0.58        55\n",
            "   macro avg       0.58      0.58      0.58        55\n",
            "weighted avg       0.58      0.58      0.58        55\n",
            "\n",
            "['0.61', '0.59', '0.60', '29']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from scipy import stats\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "results_df = pd.DataFrame(columns=['Accuracy'])\n",
        "for i in range(31):\n",
        "    X_train, X_test, y_train_v, y_test_v = train_test_split(dataframes_file[i].iloc[:,:], Y_v, test_size=0.2,random_state=21,shuffle=True)\n",
        "    acc_val = []\n",
        "    for i in range(1,100):\n",
        "        knn = KNeighborsClassifier(n_neighbors=i)\n",
        "\n",
        "        # Train the classifier\n",
        "        knn.fit(X_train, y_train_v)\n",
        "\n",
        "        # Make predictions on the test set\n",
        "        y_pred = knn.predict(X_test)\n",
        "\n",
        "        accuracy_v = accuracy_score(y_test_v, y_pred)\n",
        "        acc_val.append(accuracy_v)\n",
        "\n",
        "    acc_val = np.asarray(acc_val)\n",
        "    index = acc_val.argmax()\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=index+1)\n",
        "\n",
        "\n",
        "    # Train the classifier\n",
        "    start_time = time.time()\n",
        "    knn.fit(X_train, y_train_v)\n",
        "    end_time = time.time()\n",
        "\n",
        "    execution_time = end_time - start_time\n",
        "    # knn.fit(X_train, y_train_v)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = knn.predict(X_test)\n",
        "\n",
        "    # Calculate the accuracy\n",
        "    accuracy_v = accuracy_score(y_test_v, y_pred)\n",
        "    print(y_test_v, y_pred)\n",
        "    print(f\"Accuracy(HVLV): {accuracy_v}\")\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test_v, y_pred))\n",
        "\n",
        "    # new_row = {'Column1': 1, 'Column2': 'A', 'Column3': 10.5}\n",
        "    # empty_df = empty_df.append(new_row, ignore_index=True)\n",
        "    report = classification_report(y_test_v, y_pred)\n",
        "    print(report.split('\\n')[-7].split()[1:5])\n",
        "    precision, recall, fscore, support = map(float, report.split('\\n')[-6].split()[1:5])\n",
        "    precisionhv, recallhv, fscorehv, support = map(float, report.split('\\n')[-7].split()[1:5])\n",
        "    precisionmv, recallmv, fscoremv, support = map(float, report.split('\\n')[-3].split()[2:6])\n",
        "    precisionwv, recallwv, fscorewv, support = map(float, report.split('\\n')[-2].split()[2:6])\n",
        "    results_df = results_df.append({'Accuracy': accuracy_v,'n_neighbors': index, 'time': execution_time, 'Precision[LA]': precision, 'Recall[LA]': recall, \"F-score[LA]\": fscore, 'Precision[HA]': precisionhv, 'Recall[HA]': recallhv, \"F-score[HA]\": fscorehv,'Precision[MA]':precisionmv, 'Recall[MA]':recallmv, 'F-score[MA]':fscoremv,'Precision[WA]':precisionwv, 'Recall[WA]':recallwv, 'F-score[WA]':fscorewv},ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h-uoLrDyXY-c",
        "outputId": "7f0af09d-ab92-4af9-c7c7-9f693d8ceb11"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 31,\n  \"fields\": [\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.038250183175115464,\n        \"min\": 0.509090909090909,\n        \"max\": 0.6545454545454545,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.509090909090909,\n          0.5636363636363636,\n          0.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F-score[HA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10589099690628308,\n        \"min\": 0.19,\n        \"max\": 0.72,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0.46,\n          0.45,\n          0.19\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F-score[LA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06068427016254946,\n        \"min\": 0.48,\n        \"max\": 0.71,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.7,\n          0.59,\n          0.58\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F-score[MA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.047825848951764456,\n        \"min\": 0.43,\n        \"max\": 0.65,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.57,\n          0.65,\n          0.58\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F-score[WA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0500730648947814,\n        \"min\": 0.41,\n        \"max\": 0.65,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.41,\n          0.5,\n          0.58\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision[HA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10530855840395062,\n        \"min\": 0.55,\n        \"max\": 1.0,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.9,\n          0.73,\n          0.57\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision[LA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04491090343459845,\n        \"min\": 0.49,\n        \"max\": 0.71,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.62,\n          0.49,\n          0.56\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision[MA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05587813160260327,\n        \"min\": 0.52,\n        \"max\": 0.75,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.73,\n          0.57,\n          0.58\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision[WA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.058755919399802854,\n        \"min\": 0.52,\n        \"max\": 0.76,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          0.74,\n          0.57,\n          0.66\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall[HA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16229470075289926,\n        \"min\": 0.1,\n        \"max\": 0.83,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.31,\n          0.48,\n          0.55\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall[LA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16037322062137782,\n        \"min\": 0.38,\n        \"max\": 1.0,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.77,\n          0.38,\n          0.96\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall[MA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.036819817011104,\n        \"min\": 0.52,\n        \"max\": 0.65,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.52,\n          0.61,\n          0.64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall[WA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.038268996969247286,\n        \"min\": 0.51,\n        \"max\": 0.65,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.51,\n          0.56,\n          0.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_neighbors\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.990732842489919,\n        \"min\": 0.0,\n        \"max\": 51.0,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          19.0,\n          23.0,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.001284063009383412,\n        \"min\": 0.0018837451934814453,\n        \"max\": 0.008933782577514648,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          0.0026497840881347656,\n          0.008933782577514648,\n          0.00209808349609375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "results_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-7188e8aa-b41d-405a-8172-ddaac9d61379\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F-score[HA]</th>\n",
              "      <th>F-score[LA]</th>\n",
              "      <th>F-score[MA]</th>\n",
              "      <th>F-score[WA]</th>\n",
              "      <th>Precision[HA]</th>\n",
              "      <th>Precision[LA]</th>\n",
              "      <th>Precision[MA]</th>\n",
              "      <th>Precision[WA]</th>\n",
              "      <th>Recall[HA]</th>\n",
              "      <th>Recall[LA]</th>\n",
              "      <th>Recall[MA]</th>\n",
              "      <th>Recall[WA]</th>\n",
              "      <th>n_neighbors</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.618182</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.62</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.002120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.563636</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.56</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.002291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.654545</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.65</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.002110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.581818</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.58</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.001968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.64</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.002108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.581818</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.581818</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.002104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.002185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.527273</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.53</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.002128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.64</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.001964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.001884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.64</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.001996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.618182</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.581818</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.58</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.002115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.581818</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.581818</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.002076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.002038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.527273</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.41</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.10</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.53</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.002357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.654545</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.65</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.002065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.60</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.002143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.509091</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.51</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0.002072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.581818</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.002329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.563636</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.56</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.002098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.654545</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.65</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.002143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.563636</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.002247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.60</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.002650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.002093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.581818</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.58</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.002283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.581818</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.002206</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7188e8aa-b41d-405a-8172-ddaac9d61379')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7188e8aa-b41d-405a-8172-ddaac9d61379 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7188e8aa-b41d-405a-8172-ddaac9d61379');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8ca92fe7-db4a-4879-b28b-df4334f693a4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8ca92fe7-db4a-4879-b28b-df4334f693a4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8ca92fe7-db4a-4879-b28b-df4334f693a4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    Accuracy  F-score[HA]  F-score[LA]  F-score[MA]  F-score[WA]  \\\n",
              "0   0.618182         0.46         0.70         0.58         0.58   \n",
              "1   0.563636         0.54         0.59         0.56         0.56   \n",
              "2   0.654545         0.72         0.56         0.64         0.64   \n",
              "3   0.581818         0.53         0.62         0.58         0.57   \n",
              "4   0.636364         0.52         0.71         0.61         0.61   \n",
              "5   0.581818         0.58         0.58         0.58         0.58   \n",
              "6   0.581818         0.53         0.62         0.58         0.57   \n",
              "7   0.636364         0.63         0.64         0.64         0.64   \n",
              "8   0.600000         0.58         0.62         0.60         0.60   \n",
              "9   0.527273         0.35         0.63         0.49         0.48   \n",
              "10  0.636364         0.58         0.68         0.63         0.63   \n",
              "11  0.600000         0.66         0.52         0.59         0.59   \n",
              "12  0.636364         0.50         0.71         0.61         0.60   \n",
              "13  0.618182         0.59         0.64         0.62         0.61   \n",
              "14  0.581818         0.51         0.63         0.57         0.57   \n",
              "15  0.581818         0.55         0.61         0.58         0.58   \n",
              "16  0.581818         0.60         0.57         0.58         0.58   \n",
              "17  0.600000         0.68         0.48         0.58         0.58   \n",
              "18  0.527273         0.19         0.67         0.43         0.41   \n",
              "19  0.654545         0.68         0.63         0.65         0.65   \n",
              "20  0.600000         0.56         0.63         0.60         0.59   \n",
              "21  0.509091         0.45         0.56         0.50         0.50   \n",
              "22  0.581818         0.62         0.53         0.58         0.58   \n",
              "23  0.563636         0.45         0.64         0.55         0.54   \n",
              "24  0.654545         0.67         0.64         0.65         0.65   \n",
              "25  0.545455         0.58         0.51         0.54         0.54   \n",
              "26  0.563636         0.60         0.52         0.56         0.56   \n",
              "27  0.600000         0.50         0.67         0.58         0.58   \n",
              "28  0.636364         0.62         0.66         0.64         0.63   \n",
              "29  0.581818         0.44         0.67         0.55         0.55   \n",
              "30  0.581818         0.60         0.57         0.58         0.58   \n",
              "\n",
              "    Precision[HA]  Precision[LA]  Precision[MA]  Precision[WA]  Recall[HA]  \\\n",
              "0            0.90           0.56           0.73           0.74        0.31   \n",
              "1            0.61           0.53           0.57           0.57        0.48   \n",
              "2            0.63           0.71           0.67           0.67        0.83   \n",
              "3            0.65           0.54           0.60           0.60        0.45   \n",
              "4            0.85           0.57           0.71           0.72        0.38   \n",
              "5            0.62           0.55           0.58           0.59        0.55   \n",
              "6            0.65           0.54           0.60           0.60        0.45   \n",
              "7            0.68           0.60           0.64           0.64        0.59   \n",
              "8            0.65           0.56           0.61           0.61        0.52   \n",
              "9            0.64           0.50           0.57           0.57        0.24   \n",
              "10           0.74           0.58           0.66           0.66        0.48   \n",
              "11           0.60           0.60           0.60           0.60        0.72   \n",
              "12           0.91           0.57           0.74           0.75        0.34   \n",
              "13           0.68           0.58           0.63           0.63        0.52   \n",
              "14           0.67           0.54           0.60           0.61        0.41   \n",
              "15           0.64           0.55           0.59           0.59        0.48   \n",
              "16           0.61           0.56           0.58           0.58        0.59   \n",
              "17           0.59           0.62           0.61           0.61        0.79   \n",
              "18           1.00           0.50           0.75           0.76        0.10   \n",
              "19           0.67           0.64           0.65           0.65        0.69   \n",
              "20           0.67           0.56           0.61           0.62        0.48   \n",
              "21           0.55           0.49           0.52           0.52        0.38   \n",
              "22           0.59           0.57           0.58           0.58        0.66   \n",
              "23           0.67           0.53           0.60           0.60        0.34   \n",
              "24           0.68           0.63           0.65           0.66        0.66   \n",
              "25           0.57           0.52           0.54           0.54        0.59   \n",
              "26           0.58           0.54           0.56           0.56        0.62   \n",
              "27           0.73           0.55           0.64           0.65        0.38   \n",
              "28           0.70           0.59           0.64           0.65        0.55   \n",
              "29           0.75           0.53           0.64           0.65        0.31   \n",
              "30           0.61           0.56           0.58           0.58        0.59   \n",
              "\n",
              "    Recall[LA]  Recall[MA]  Recall[WA]  n_neighbors      time  \n",
              "0         0.96        0.64        0.62         19.0  0.002120  \n",
              "1         0.65        0.57        0.56         23.0  0.002291  \n",
              "2         0.46        0.64        0.65          1.0  0.002110  \n",
              "3         0.73        0.59        0.58         19.0  0.001968  \n",
              "4         0.92        0.65        0.64         15.0  0.002108  \n",
              "5         0.62        0.58        0.58          0.0  0.002034  \n",
              "6         0.73        0.59        0.58          0.0  0.002158  \n",
              "7         0.69        0.64        0.64          3.0  0.002104  \n",
              "8         0.69        0.60        0.60          8.0  0.002185  \n",
              "9         0.85        0.54        0.53          6.0  0.002128  \n",
              "10        0.81        0.65        0.64         17.0  0.001964  \n",
              "11        0.46        0.59        0.60          1.0  0.001884  \n",
              "12        0.96        0.65        0.64         27.0  0.001996  \n",
              "13        0.73        0.62        0.62          0.0  0.004446  \n",
              "14        0.77        0.59        0.58          4.0  0.002115  \n",
              "15        0.69        0.59        0.58          0.0  0.008934  \n",
              "16        0.58        0.58        0.58          3.0  0.002076  \n",
              "17        0.38        0.59        0.60          1.0  0.002038  \n",
              "18        1.00        0.55        0.53         21.0  0.002357  \n",
              "19        0.62        0.65        0.65          3.0  0.002065  \n",
              "20        0.73        0.61        0.60          2.0  0.002143  \n",
              "21        0.65        0.52        0.51         19.0  0.002072  \n",
              "22        0.50        0.58        0.58          3.0  0.002329  \n",
              "23        0.81        0.58        0.56          8.0  0.002098  \n",
              "24        0.65        0.65        0.65         17.0  0.002143  \n",
              "25        0.50        0.54        0.55          0.0  0.002012  \n",
              "26        0.50        0.56        0.56          5.0  0.002247  \n",
              "27        0.85        0.61        0.60         51.0  0.002650  \n",
              "28        0.73        0.64        0.64         45.0  0.002093  \n",
              "29        0.88        0.60        0.58         23.0  0.002283  \n",
              "30        0.58        0.58        0.58          7.0  0.002206  "
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qje-wnwSXbkW"
      },
      "outputs": [],
      "source": [
        "ch_names = ['Cz','FP1','AF3','F7','F3','Fz','FC5','FC1','T7','C3','CP5','CP1','P7','P3','PO3','O1','FP2','AF4','F8','F4','FC6','FC2','T8','C4','CP6','CP2','P8','P4','Pz','PO4','O2','Oz']\n",
        "results_df['ch_names'] = ch_names[1:]\n",
        "results_df.to_csv('/content/drive/MyDrive/dense/rhythms_418_time/selected_v/84_features_valence_knn.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w65c3ypWXiaG",
        "outputId": "8fabd139-bec4-4048-f5e0-a598cf69bb1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1]\n",
            "Accuracy(HALA): 0.5818181818181818\n",
            "Precision: 0.6254355400696865\n",
            "Recall: 0.5954907161803713\n",
            "F1 Score: 0.5609163484901076\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.34      0.47        29\n",
            "           1       0.54      0.85      0.66        26\n",
            "\n",
            "    accuracy                           0.58        55\n",
            "   macro avg       0.63      0.60      0.56        55\n",
            "weighted avg       0.63      0.58      0.56        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1]\n",
            "Accuracy(HALA): 0.6\n",
            "Precision: 0.6416666666666666\n",
            "Recall: 0.6127320954907162\n",
            "F1 Score: 0.5833333333333334\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.38      0.50        29\n",
            "           1       0.55      0.85      0.67        26\n",
            "\n",
            "    accuracy                           0.60        55\n",
            "   macro avg       0.64      0.61      0.58        55\n",
            "weighted avg       0.65      0.60      0.58        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0]\n",
            "Accuracy(HALA): 0.5272727272727272\n",
            "Precision: 0.5441176470588236\n",
            "Recall: 0.5377984084880636\n",
            "F1 Score: 0.514266304347826\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.34      0.43        29\n",
            "           1       0.50      0.73      0.59        26\n",
            "\n",
            "    accuracy                           0.53        55\n",
            "   macro avg       0.54      0.54      0.51        55\n",
            "weighted avg       0.55      0.53      0.51        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0]\n",
            "Accuracy(HALA): 0.6181818181818182\n",
            "Precision: 0.6733449477351916\n",
            "Recall: 0.6319628647214854\n",
            "F1 Score: 0.5990975355779242\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.38      0.51        29\n",
            "           1       0.56      0.88      0.69        26\n",
            "\n",
            "    accuracy                           0.62        55\n",
            "   macro avg       0.67      0.63      0.60        55\n",
            "weighted avg       0.68      0.62      0.59        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1]\n",
            "Accuracy(HALA): 0.509090909090909\n",
            "Precision: 0.5296167247386759\n",
            "Recall: 0.5225464190981433\n",
            "F1 Score: 0.48455397431447417\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.28      0.37        29\n",
            "           1       0.49      0.77      0.60        26\n",
            "\n",
            "    accuracy                           0.51        55\n",
            "   macro avg       0.53      0.52      0.48        55\n",
            "weighted avg       0.53      0.51      0.48        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            "Accuracy(HALA): 0.6545454545454545\n",
            "Precision: 0.6606182795698925\n",
            "Recall: 0.6584880636604774\n",
            "F1 Score: 0.6540880503144655\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.59      0.64        29\n",
            "           1       0.61      0.73      0.67        26\n",
            "\n",
            "    accuracy                           0.65        55\n",
            "   macro avg       0.66      0.66      0.65        55\n",
            "weighted avg       0.66      0.65      0.65        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0]\n",
            "Accuracy(HALA): 0.5636363636363636\n",
            "Precision: 0.586687306501548\n",
            "Recall: 0.5742705570291777\n",
            "F1 Score: 0.5516304347826086\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.38      0.48        29\n",
            "           1       0.53      0.77      0.62        26\n",
            "\n",
            "    accuracy                           0.56        55\n",
            "   macro avg       0.59      0.57      0.55        55\n",
            "weighted avg       0.59      0.56      0.55        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0]\n",
            "Accuracy(HALA): 0.6363636363636364\n",
            "Precision: 0.6600877192982456\n",
            "Recall: 0.6452254641909815\n",
            "F1 Score: 0.6303763440860215\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.48      0.58        29\n",
            "           1       0.58      0.81      0.68        26\n",
            "\n",
            "    accuracy                           0.64        55\n",
            "   macro avg       0.66      0.65      0.63        55\n",
            "weighted avg       0.66      0.64      0.63        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0]\n",
            "Accuracy(HALA): 0.6\n",
            "Precision: 0.6198830409356726\n",
            "Recall: 0.6087533156498675\n",
            "F1 Score: 0.5934139784946236\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.45      0.54        29\n",
            "           1       0.56      0.77      0.65        26\n",
            "\n",
            "    accuracy                           0.60        55\n",
            "   macro avg       0.62      0.61      0.59        55\n",
            "weighted avg       0.62      0.60      0.59        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1]\n",
            "Accuracy(HALA): 0.509090909090909\n",
            "Precision: 0.5444444444444444\n",
            "Recall: 0.5265251989389921\n",
            "F1 Score: 0.4637053087757313\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.21      0.31        29\n",
            "           1       0.49      0.85      0.62        26\n",
            "\n",
            "    accuracy                           0.51        55\n",
            "   macro avg       0.54      0.53      0.46        55\n",
            "weighted avg       0.55      0.51      0.46        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0]\n",
            "Accuracy(HALA): 0.5818181818181818\n",
            "Precision: 0.6129807692307692\n",
            "Recall: 0.5935013262599469\n",
            "F1 Score: 0.5675213675213675\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.38      0.49        29\n",
            "           1       0.54      0.81      0.65        26\n",
            "\n",
            "    accuracy                           0.58        55\n",
            "   macro avg       0.61      0.59      0.57        55\n",
            "weighted avg       0.62      0.58      0.56        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0\n",
            " 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 0]\n",
            "Accuracy(HALA): 0.5454545454545454\n",
            "Precision: 0.5775261324041812\n",
            "Recall: 0.5590185676392573\n",
            "F1 Score: 0.5227351614022909\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.31      0.42        29\n",
            "           1       0.51      0.81      0.63        26\n",
            "\n",
            "    accuracy                           0.55        55\n",
            "   macro avg       0.58      0.56      0.52        55\n",
            "weighted avg       0.58      0.55      0.52        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0\n",
            " 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1]\n",
            "Accuracy(HALA): 0.5636363636363636\n",
            "Precision: 0.5742296918767507\n",
            "Recall: 0.5702917771883289\n",
            "F1 Score: 0.5599999999999999\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.45      0.52        29\n",
            "           1       0.53      0.69      0.60        26\n",
            "\n",
            "    accuracy                           0.56        55\n",
            "   macro avg       0.57      0.57      0.56        55\n",
            "weighted avg       0.58      0.56      0.56        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 0]\n",
            "Accuracy(HALA): 0.5272727272727272\n",
            "Precision: 0.5576923076923077\n",
            "Recall: 0.5417771883289124\n",
            "F1 Score: 0.4992997198879552\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.28      0.38        29\n",
            "           1       0.50      0.81      0.62        26\n",
            "\n",
            "    accuracy                           0.53        55\n",
            "   macro avg       0.56      0.54      0.50        55\n",
            "weighted avg       0.56      0.53      0.49        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1]\n",
            "Accuracy(HALA): 0.509090909090909\n",
            "Precision: 0.5178571428571429\n",
            "Recall: 0.51657824933687\n",
            "F1 Score: 0.5031783205085313\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.38      0.45        29\n",
            "           1       0.49      0.65      0.56        26\n",
            "\n",
            "    accuracy                           0.51        55\n",
            "   macro avg       0.52      0.52      0.50        55\n",
            "weighted avg       0.52      0.51      0.50        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0\n",
            " 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0]\n",
            "Accuracy(HALA): 0.5636363636363636\n",
            "Precision: 0.5796783625730995\n",
            "Recall: 0.5722811671087533\n",
            "F1 Score: 0.5564516129032258\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.41      0.50        29\n",
            "           1       0.53      0.73      0.61        26\n",
            "\n",
            "    accuracy                           0.56        55\n",
            "   macro avg       0.58      0.57      0.56        55\n",
            "weighted avg       0.58      0.56      0.55        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0]\n",
            "Accuracy(HALA): 0.5636363636363636\n",
            "Precision: 0.6080586080586081\n",
            "Recall: 0.5782493368700266\n",
            "F1 Score: 0.5378151260504203\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.31      0.43        29\n",
            "           1       0.52      0.85      0.65        26\n",
            "\n",
            "    accuracy                           0.56        55\n",
            "   macro avg       0.61      0.58      0.54        55\n",
            "weighted avg       0.61      0.56      0.53        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0]\n",
            "Accuracy(HALA): 0.6363636363636364\n",
            "Precision: 0.6447010869565217\n",
            "Recall: 0.6412466843501325\n",
            "F1 Score: 0.6352785145888593\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.55      0.62        29\n",
            "           1       0.59      0.73      0.66        26\n",
            "\n",
            "    accuracy                           0.64        55\n",
            "   macro avg       0.64      0.64      0.64        55\n",
            "weighted avg       0.65      0.64      0.63        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0]\n",
            "Accuracy(HALA): 0.4909090909090909\n",
            "Precision: 0.4992690058479532\n",
            "Recall: 0.4993368700265252\n",
            "F1 Score: 0.4825268817204301\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.34      0.42        29\n",
            "           1       0.47      0.65      0.55        26\n",
            "\n",
            "    accuracy                           0.49        55\n",
            "   macro avg       0.50      0.50      0.48        55\n",
            "weighted avg       0.50      0.49      0.48        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1]\n",
            "Accuracy(HALA): 0.5454545454545454\n",
            "Precision: 0.5891472868217054\n",
            "Recall: 0.5610079575596817\n",
            "F1 Score: 0.5139625309296572\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.28      0.39        29\n",
            "           1       0.51      0.85      0.64        26\n",
            "\n",
            "    accuracy                           0.55        55\n",
            "   macro avg       0.59      0.56      0.51        55\n",
            "weighted avg       0.59      0.55      0.51        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1]\n",
            "Accuracy(HALA): 0.6181818181818182\n",
            "Precision: 0.6448948948948949\n",
            "Recall: 0.6279840848806366\n",
            "F1 Score: 0.6099290780141844\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.45      0.55        29\n",
            "           1       0.57      0.81      0.67        26\n",
            "\n",
            "    accuracy                           0.62        55\n",
            "   macro avg       0.64      0.63      0.61        55\n",
            "weighted avg       0.65      0.62      0.61        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1\n",
            " 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0]\n",
            "Accuracy(HALA): 0.6363636363636364\n",
            "Precision: 0.6600877192982456\n",
            "Recall: 0.6452254641909815\n",
            "F1 Score: 0.6303763440860215\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.48      0.58        29\n",
            "           1       0.58      0.81      0.68        26\n",
            "\n",
            "    accuracy                           0.64        55\n",
            "   macro avg       0.66      0.65      0.63        55\n",
            "weighted avg       0.66      0.64      0.63        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0]\n",
            "Accuracy(HALA): 0.5636363636363636\n",
            "Precision: 0.5958333333333333\n",
            "Recall: 0.5762599469496021\n",
            "F1 Score: 0.5454545454545454\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.34      0.45        29\n",
            "           1       0.53      0.81      0.64        26\n",
            "\n",
            "    accuracy                           0.56        55\n",
            "   macro avg       0.60      0.58      0.55        55\n",
            "weighted avg       0.60      0.56      0.54        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0]\n",
            "Accuracy(HALA): 0.509090909090909\n",
            "Precision: 0.5210210210210211\n",
            "Recall: 0.5185676392572944\n",
            "F1 Score: 0.49848024316109424\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.34      0.43        29\n",
            "           1       0.49      0.69      0.57        26\n",
            "\n",
            "    accuracy                           0.51        55\n",
            "   macro avg       0.52      0.52      0.50        55\n",
            "weighted avg       0.52      0.51      0.49        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 0]\n",
            "Accuracy(HALA): 0.6545454545454545\n",
            "Precision: 0.675\n",
            "Recall: 0.6624668435013263\n",
            "F1 Score: 0.6503847440615591\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.52      0.61        29\n",
            "           1       0.60      0.81      0.69        26\n",
            "\n",
            "    accuracy                           0.65        55\n",
            "   macro avg       0.68      0.66      0.65        55\n",
            "weighted avg       0.68      0.65      0.65        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            "Accuracy(HALA): 0.6\n",
            "Precision: 0.5988063660477454\n",
            "Recall: 0.5988063660477454\n",
            "F1 Score: 0.5988063660477454\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.62      0.62        29\n",
            "           1       0.58      0.58      0.58        26\n",
            "\n",
            "    accuracy                           0.60        55\n",
            "   macro avg       0.60      0.60      0.60        55\n",
            "weighted avg       0.60      0.60      0.60        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1\n",
            " 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0]\n",
            "Accuracy(HALA): 0.6545454545454545\n",
            "Precision: 0.6541005291005291\n",
            "Recall: 0.6545092838196287\n",
            "F1 Score: 0.6540880503144654\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.66      0.67        29\n",
            "           1       0.63      0.65      0.64        26\n",
            "\n",
            "    accuracy                           0.65        55\n",
            "   macro avg       0.65      0.65      0.65        55\n",
            "weighted avg       0.66      0.65      0.65        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0]\n",
            "Accuracy(HALA): 0.6363636363636364\n",
            "Precision: 0.6512605042016807\n",
            "Recall: 0.6432360742705571\n",
            "F1 Score: 0.6333333333333334\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.52      0.60        29\n",
            "           1       0.59      0.77      0.67        26\n",
            "\n",
            "    accuracy                           0.64        55\n",
            "   macro avg       0.65      0.64      0.63        55\n",
            "weighted avg       0.65      0.64      0.63        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0]\n",
            "Accuracy(HALA): 0.5636363636363636\n",
            "Precision: 0.5958333333333333\n",
            "Recall: 0.5762599469496021\n",
            "F1 Score: 0.5454545454545454\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.34      0.45        29\n",
            "           1       0.53      0.81      0.64        26\n",
            "\n",
            "    accuracy                           0.56        55\n",
            "   macro avg       0.60      0.58      0.55        55\n",
            "weighted avg       0.60      0.56      0.54        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0]\n",
            "Accuracy(HALA): 0.6\n",
            "Precision: 0.6127450980392157\n",
            "Recall: 0.6067639257294429\n",
            "F1 Score: 0.5966666666666667\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.48      0.56        29\n",
            "           1       0.56      0.73      0.63        26\n",
            "\n",
            "    accuracy                           0.60        55\n",
            "   macro avg       0.61      0.61      0.60        55\n",
            "weighted avg       0.62      0.60      0.59        55\n",
            "\n",
            "[0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0] [1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1]\n",
            "Accuracy(HALA): 0.6727272727272727\n",
            "Precision: 0.6897759103641457\n",
            "Recall: 0.6797082228116711\n",
            "F1 Score: 0.6699999999999999\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.55      0.64        29\n",
            "           1       0.62      0.81      0.70        26\n",
            "\n",
            "    accuracy                           0.67        55\n",
            "   macro avg       0.69      0.68      0.67        55\n",
            "weighted avg       0.69      0.67      0.67        55\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "results_df_a = pd.DataFrame(columns=['Accuracy'])\n",
        "\n",
        "for i in range(31):\n",
        "    X_train, X_test, y_train_a, y_test_a = train_test_split(dataframes_file[i].iloc[:,:], Y_v, test_size=0.2, random_state=21, shuffle=True)\n",
        "    acc_val = []\n",
        "    for i in range(1,200):\n",
        "        xgb = XGBClassifier(n_estimators=i)\n",
        "\n",
        "        # Train the classifier\n",
        "        xgb.fit(X_train, y_train_a)\n",
        "\n",
        "        # Make predictions on the test set\n",
        "        y_pred = xgb.predict(X_test)\n",
        "\n",
        "        accuracy_a = accuracy_score(y_test_a, y_pred)\n",
        "        acc_val.append(accuracy_a)\n",
        "\n",
        "    acc_val = np.asarray(acc_val)\n",
        "    index = acc_val.argmax()\n",
        "\n",
        "    # Create an XGBoost classifier\n",
        "    xgb = XGBClassifier(n_estimators=index+1, random_state=21)\n",
        "\n",
        "    # Train the classifier\n",
        "    start_time = time.time()\n",
        "    xgb.fit(X_train, y_train_a)\n",
        "    end_time = time.time()\n",
        "\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = xgb.predict(X_test)\n",
        "\n",
        "    # Calculate the accuracy\n",
        "    accuracy_a = accuracy_score(y_test_a, y_pred)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test_a, y_pred, average='macro')\n",
        "\n",
        "    print(y_test_a, y_pred)\n",
        "    print(f\"Accuracy(HALA): {accuracy_a}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1_score}\")\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test_a, y_pred))\n",
        "\n",
        "    # Append the results to the results dataframe\n",
        "    report = classification_report(y_test_a, y_pred)\n",
        "    precision, recall, fscore, support = map(float, report.split('\\n')[-6].split()[1:5])\n",
        "    precisionhv, recallhv, fscorehv, support = map(float, report.split('\\n')[-7].split()[1:5])\n",
        "    precisionmv, recallmv, fscoremv, support = map(float, report.split('\\n')[-3].split()[2:6])\n",
        "    precisionwv, recallwv, fscorewv, support = map(float, report.split('\\n')[-2].split()[2:6])\n",
        "    results_df_a = results_df_a.append({'Accuracy': accuracy_a, 'n_estimators': index, 'time': execution_time, 'Precision[LA]': precision, 'Recall[LA]': recall, \"F-score[LA]\": fscore, 'Precision[HA]': precisionhv, 'Recall[HA]': recallhv, \"F-score[HA]\": fscorehv,'Precision[MA]':precisionmv, 'Recall[MA]':recallmv, 'F-score[MA]':fscoremv,'Precision[WA]':precisionwv, 'Recall[WA]':recallwv, 'F-score[WA]':fscorewv},ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2SxAZX_fX2O7",
        "outputId": "02dcaa37-93fb-418a-ae4d-5eed87960e05"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"results_df_a\",\n  \"rows\": 31,\n  \"fields\": [\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05163977794943224,\n        \"min\": 0.4909090909090909,\n        \"max\": 0.6727272727272727,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.6545454545454545,\n          0.5818181818181818,\n          0.4909090909090909\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F-score[HA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09228753570938943,\n        \"min\": 0.31,\n        \"max\": 0.67,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          0.47,\n          0.38,\n          0.54\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F-score[LA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.039627295903772716,\n        \"min\": 0.55,\n        \"max\": 0.7,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.66,\n          0.67,\n          0.62\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F-score[MA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05908987140979244,\n        \"min\": 0.46,\n        \"max\": 0.67,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.56,\n          0.58,\n          0.65\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F-score[WA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.059712573197354867,\n        \"min\": 0.46,\n        \"max\": 0.67,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.56,\n          0.58,\n          0.65\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision[HA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06555142444914396,\n        \"min\": 0.53,\n        \"max\": 0.79,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0.71,\n          0.72,\n          0.53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision[LA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04288268568061935,\n        \"min\": 0.47,\n        \"max\": 0.63,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.54,\n          0.55,\n          0.61\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision[MA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05143531275154709,\n        \"min\": 0.5,\n        \"max\": 0.69,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          0.63,\n          0.64,\n          0.61\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision[WA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.052071084581198857,\n        \"min\": 0.5,\n        \"max\": 0.69,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.56,\n          0.61,\n          0.63\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall[HA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11047015847720366,\n        \"min\": 0.21,\n        \"max\": 0.66,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.62,\n          0.55,\n          0.34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall[LA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07307530362578045,\n        \"min\": 0.58,\n        \"max\": 0.88,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.73,\n          0.69,\n          0.85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall[MA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04928957664244438,\n        \"min\": 0.5,\n        \"max\": 0.68,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.59,\n          0.58,\n          0.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall[WA]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05125730999532641,\n        \"min\": 0.49,\n        \"max\": 0.67,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.65,\n          0.58,\n          0.49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_estimators\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 43.73967466482743,\n        \"min\": 0.0,\n        \"max\": 192.0,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          0.0,\n          1.0,\n          42.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11173753246370316,\n        \"min\": 0.021178722381591797,\n        \"max\": 0.5227150917053223,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          0.08165431022644043,\n          0.03243255615234375,\n          0.026262760162353516\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "results_df_a"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-2a8674b8-c89c-4f7e-a278-616b293ad74b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F-score[HA]</th>\n",
              "      <th>F-score[LA]</th>\n",
              "      <th>F-score[MA]</th>\n",
              "      <th>F-score[WA]</th>\n",
              "      <th>Precision[HA]</th>\n",
              "      <th>Precision[LA]</th>\n",
              "      <th>Precision[MA]</th>\n",
              "      <th>Precision[WA]</th>\n",
              "      <th>Recall[HA]</th>\n",
              "      <th>Recall[LA]</th>\n",
              "      <th>Recall[MA]</th>\n",
              "      <th>Recall[WA]</th>\n",
              "      <th>n_estimators</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.581818</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.025072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.039750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.527273</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.53</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.035819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.618182</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.62</td>\n",
              "      <td>155.0</td>\n",
              "      <td>0.260257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.509091</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.51</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.117917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.654545</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.65</td>\n",
              "      <td>70.0</td>\n",
              "      <td>0.196190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.563636</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.56</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.059656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.64</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.522715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.60</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.039076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.509091</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.51</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.028434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.581818</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.58</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.309248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.55</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.026435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.563636</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.56</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.149142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.527273</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.53</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.030865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.509091</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.51</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.070340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.563636</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.56</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.032433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.563636</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.56</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.041080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.043104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.490909</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.49</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.066470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.021179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.618182</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.62</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.129228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.64</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.056838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.563636</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.022475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.509091</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.51</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.026263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.654545</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.65</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.111918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.036968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.654545</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.65</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.284382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.081654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.563636</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.56</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.062112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.60</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.034523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.672727</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.67</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.046670</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a8674b8-c89c-4f7e-a278-616b293ad74b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2a8674b8-c89c-4f7e-a278-616b293ad74b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2a8674b8-c89c-4f7e-a278-616b293ad74b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3189afbe-e375-458e-867c-c2760bf35ec1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3189afbe-e375-458e-867c-c2760bf35ec1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3189afbe-e375-458e-867c-c2760bf35ec1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    Accuracy  F-score[HA]  F-score[LA]  F-score[MA]  F-score[WA]  \\\n",
              "0   0.581818         0.47         0.66         0.56         0.56   \n",
              "1   0.600000         0.50         0.67         0.58         0.58   \n",
              "2   0.527273         0.43         0.59         0.51         0.51   \n",
              "3   0.618182         0.51         0.69         0.60         0.59   \n",
              "4   0.509091         0.37         0.60         0.48         0.48   \n",
              "5   0.654545         0.64         0.67         0.65         0.65   \n",
              "6   0.563636         0.48         0.62         0.55         0.55   \n",
              "7   0.636364         0.58         0.68         0.63         0.63   \n",
              "8   0.600000         0.54         0.65         0.59         0.59   \n",
              "9   0.509091         0.31         0.62         0.46         0.46   \n",
              "10  0.581818         0.49         0.65         0.57         0.56   \n",
              "11  0.545455         0.42         0.63         0.52         0.52   \n",
              "12  0.563636         0.52         0.60         0.56         0.56   \n",
              "13  0.527273         0.38         0.62         0.50         0.49   \n",
              "14  0.509091         0.45         0.56         0.50         0.50   \n",
              "15  0.563636         0.50         0.61         0.56         0.55   \n",
              "16  0.563636         0.43         0.65         0.54         0.53   \n",
              "17  0.636364         0.62         0.66         0.64         0.63   \n",
              "18  0.490909         0.42         0.55         0.48         0.48   \n",
              "19  0.545455         0.39         0.64         0.51         0.51   \n",
              "20  0.618182         0.55         0.67         0.61         0.61   \n",
              "21  0.636364         0.58         0.68         0.63         0.63   \n",
              "22  0.563636         0.45         0.64         0.55         0.54   \n",
              "23  0.509091         0.43         0.57         0.50         0.49   \n",
              "24  0.654545         0.61         0.69         0.65         0.65   \n",
              "25  0.600000         0.62         0.58         0.60         0.60   \n",
              "26  0.654545         0.67         0.64         0.65         0.65   \n",
              "27  0.636364         0.60         0.67         0.63         0.63   \n",
              "28  0.563636         0.45         0.64         0.55         0.54   \n",
              "29  0.600000         0.56         0.63         0.60         0.59   \n",
              "30  0.672727         0.64         0.70         0.67         0.67   \n",
              "\n",
              "    Precision[HA]  Precision[LA]  Precision[MA]  Precision[WA]  Recall[HA]  \\\n",
              "0            0.71           0.54           0.63           0.63        0.34   \n",
              "1            0.73           0.55           0.64           0.65        0.38   \n",
              "2            0.59           0.50           0.54           0.55        0.34   \n",
              "3            0.79           0.56           0.67           0.68        0.38   \n",
              "4            0.57           0.49           0.53           0.53        0.28   \n",
              "5            0.71           0.61           0.66           0.66        0.59   \n",
              "6            0.65           0.53           0.59           0.59        0.38   \n",
              "7            0.74           0.58           0.66           0.66        0.48   \n",
              "8            0.68           0.56           0.62           0.62        0.45   \n",
              "9            0.60           0.49           0.54           0.55        0.21   \n",
              "10           0.69           0.54           0.61           0.62        0.38   \n",
              "11           0.64           0.51           0.58           0.58        0.31   \n",
              "12           0.62           0.53           0.57           0.58        0.45   \n",
              "13           0.62           0.50           0.56           0.56        0.28   \n",
              "14           0.55           0.49           0.52           0.52        0.38   \n",
              "15           0.63           0.53           0.58           0.58        0.41   \n",
              "16           0.69           0.52           0.61           0.61        0.31   \n",
              "17           0.70           0.59           0.64           0.65        0.55   \n",
              "18           0.53           0.47           0.50           0.50        0.34   \n",
              "19           0.67           0.51           0.59           0.59        0.28   \n",
              "20           0.72           0.57           0.64           0.65        0.45   \n",
              "21           0.74           0.58           0.66           0.66        0.48   \n",
              "22           0.67           0.53           0.60           0.60        0.34   \n",
              "23           0.56           0.49           0.52           0.52        0.34   \n",
              "24           0.75           0.60           0.68           0.68        0.52   \n",
              "25           0.62           0.58           0.60           0.60        0.62   \n",
              "26           0.68           0.63           0.65           0.66        0.66   \n",
              "27           0.71           0.59           0.65           0.65        0.52   \n",
              "28           0.67           0.53           0.60           0.60        0.34   \n",
              "29           0.67           0.56           0.61           0.62        0.48   \n",
              "30           0.76           0.62           0.69           0.69        0.55   \n",
              "\n",
              "    Recall[LA]  Recall[MA]  Recall[WA]  n_estimators      time  \n",
              "0         0.85        0.60        0.58           0.0  0.025072  \n",
              "1         0.85        0.61        0.60           1.0  0.039750  \n",
              "2         0.73        0.54        0.53           3.0  0.035819  \n",
              "3         0.88        0.63        0.62         155.0  0.260257  \n",
              "4         0.77        0.52        0.51          36.0  0.117917  \n",
              "5         0.73        0.66        0.65          70.0  0.196190  \n",
              "6         0.77        0.57        0.56           8.0  0.059656  \n",
              "7         0.81        0.65        0.64           5.0  0.522715  \n",
              "8         0.77        0.61        0.60           5.0  0.039076  \n",
              "9         0.85        0.53        0.51           1.0  0.028434  \n",
              "10        0.81        0.59        0.58           8.0  0.309248  \n",
              "11        0.81        0.56        0.55           1.0  0.026435  \n",
              "12        0.69        0.57        0.56          42.0  0.149142  \n",
              "13        0.81        0.54        0.53           5.0  0.030865  \n",
              "14        0.65        0.52        0.51          17.0  0.070340  \n",
              "15        0.73        0.57        0.56           2.0  0.032433  \n",
              "16        0.85        0.58        0.56           4.0  0.041080  \n",
              "17        0.73        0.64        0.64           6.0  0.043104  \n",
              "18        0.65        0.50        0.49          12.0  0.066470  \n",
              "19        0.85        0.56        0.55           0.0  0.021179  \n",
              "20        0.81        0.63        0.62          44.0  0.129228  \n",
              "21        0.81        0.65        0.64          10.0  0.056838  \n",
              "22        0.81        0.58        0.56           0.0  0.022475  \n",
              "23        0.69        0.52        0.51           1.0  0.026263  \n",
              "24        0.81        0.66        0.65          29.0  0.111918  \n",
              "25        0.58        0.60        0.60           3.0  0.036968  \n",
              "26        0.65        0.65        0.65         192.0  0.284382  \n",
              "27        0.77        0.64        0.64          17.0  0.081654  \n",
              "28        0.81        0.58        0.56          10.0  0.062112  \n",
              "29        0.73        0.61        0.60           2.0  0.034523  \n",
              "30        0.81        0.68        0.67           3.0  0.046670  "
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbOUx-o-X4qR"
      },
      "outputs": [],
      "source": [
        "ch_names = ['Cz','FP1','AF3','F7','F3','Fz','FC5','FC1','T7','C3','CP5','CP1','P7','P3','PO3','O1','FP2','AF4','F8','F4','FC6','FC2','T8','C4','CP6','CP2','P8','P4','Pz','PO4','O2','Oz']\n",
        "results_df_a['ch_names'] = ch_names[1:]\n",
        "results_df_a.to_csv('/content/drive/MyDrive/dense/rhythms_418_time/selected_v/84_features_valence_xgb.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iwb_qaxxYCvp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
